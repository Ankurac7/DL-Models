{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Course Link](https://www.linkedin.com/learning-login/share?account=229219690&forceAccount=false&redirect=https%3A%2F%2Fwww.linkedin.com%2Flearning%2Frecurrent-neural-networks%3Ftrk%3Dshare_ent_url%26shareId%3DUHZWuIfcQP68edefm46grw%253D%253D)"
      ],
      "metadata": {
        "id": "wriNJ45EbmUt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJ3_S3NKbSlA",
        "outputId": "beff4627-f1a4-4c96-8252-898c2c7498f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded Data :\n",
            "------------------------------------\n",
            "  CLASS                                                SMS\n",
            "0   ham   said kiss, kiss, i can't do the sound effects...\n",
            "1   ham      &lt;#&gt; ISH MINUTES WAS 5 MINUTES AGO. WTF.\n",
            "2  spam  (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
            "3  spam  * FREE* POLYPHONIC RINGTONE Text SUPER to 8713...\n",
            "4  spam  **FREE MESSAGE**Thanks for using the Auction S...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "#Load Spam Data and review content\n",
        "spam_data = pd.read_csv(\"/content/Spam-Classification.csv\")\n",
        "\n",
        "print(\"\\nLoaded Data :\\n------------------------------------\")\n",
        "print(spam_data.head())\n",
        "\n",
        "#Separate feature and target data\n",
        "spam_classes_raw = spam_data[\"CLASS\"]\n",
        "spam_messages = spam_data[\"SMS\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Build a label encoder for target variable to convert strings to numeric values.\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "spam_classes = label_encoder.fit_transform(\n",
        "                                spam_classes_raw)\n",
        "\n",
        "#Convert target to one-hot encoding vector\n",
        "spam_classes = tf.keras.utils.to_categorical(spam_classes,2)\n",
        "\n",
        "print(\"One-hot Encoding Shape : \", spam_classes.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQMLu6sncN6w",
        "outputId": "7985d054-a7a3-4268-e196-0659bb2adb33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot Encoding Shape :  (1500, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess data for spam messages\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "#Max words in the vocabulary for this dataset\n",
        "VOCAB_WORDS=10000\n",
        "#Max sequence length for word sequences\n",
        "MAX_SEQUENCE_LENGTH=100\n",
        "\n",
        "#Create a vocabulary with unique words and IDs\n",
        "spam_tokenizer = Tokenizer(num_words=VOCAB_WORDS)\n",
        "spam_tokenizer.fit_on_texts(spam_messages)\n",
        "\n",
        "\n",
        "print(\"Total unique tokens found: \", len(spam_tokenizer.word_index))\n",
        "print(\"Example token ID for word \\\"me\\\" :\", spam_tokenizer.word_index.get(\"me\"))\n",
        "\n",
        "#Convert sentences to token-ID sequences\n",
        "spam_sequences = spam_tokenizer.texts_to_sequences(spam_messages)\n",
        "\n",
        "#Pad all sequences to fixed length\n",
        "spam_padded = pad_sequences(spam_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "print(\"\\nTotal sequences found : \", len(spam_padded))\n",
        "print(\"Example Sequence for sentence : \", spam_messages[0] )\n",
        "print(spam_padded[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3CmznXTcm4b",
        "outputId": "7b692452-a6ce-410a-998d-510690d84db6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique tokens found:  4688\n",
            "Example token ID for word \"me\" : 25\n",
            "\n",
            "Total sequences found :  1500\n",
            "Example Sequence for sentence :   said kiss, kiss, i can't do the sound effects! He is a gorgeous man isn't he! Kind of person who needs a smile to brighten his day! \n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0  260  921  921    4  430   55    6 1488 2294  148   10\n",
            "    3 1489  464 1143  148  922   19  514   77 1144    3  515    1 2295\n",
            "  397   89]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(spam_padded,spam_classes,test_size=0.2)"
      ],
      "metadata": {
        "id": "fQvK17VBc-hU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the pre-trained embeddings\n",
        "import numpy as np\n",
        "#Read pretrained embeddings into a dictionary\n",
        "glove_dict = {}\n",
        "\n",
        "#Loading a 50 feature (dimension) embedding with 6 billion words\n",
        "with open('glove.6B.50d.txt', \"r\", encoding=\"utf8\") as glove_file:\n",
        "    for line in glove_file:\n",
        "        emb_line = line.split()\n",
        "        emb_token = emb_line[0]\n",
        "        emb_vector = np.array(emb_line[1:], dtype=np.float32)\n",
        "\n",
        "        if emb_vector.shape[0] == 50:\n",
        "            glove_dict[emb_token] = emb_vector\n",
        "\n",
        "print(\"Dictionary Size: \", len(glove_dict))\n",
        "print(\"\\n Sample Dictionary Entry for word \\\"the\\\" :\\n\", glove_dict.get(\"the\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNzV0TJIdFgl",
        "outputId": "7c34e8b6-c691-4c7f-9f38-d8af6846017f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary Size:  34355\n",
            "\n",
            " Sample Dictionary Entry for word \"the\" :\n",
            " [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
            " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
            " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
            " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
            " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
            "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
            "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
            " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
            " -1.1514e-01 -7.8581e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We now associate each token ID in our data set vocabulary to the corresponding embedding in Glove\n",
        "#If the word is not available, then embedding will be all zeros.\n",
        "\n",
        "#Matrix with 1 row for each word in the data set vocubulary and 50 features\n",
        "\n",
        "vocab_len = len(spam_tokenizer.word_index) + 1\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_len, 50))\n",
        "\n",
        "for word, id in spam_tokenizer.word_index.items():\n",
        "    try:\n",
        "        embedding_vector = glove_dict.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[id] = embedding_vector\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print(\"Size of Embedding matrix :\", embedding_matrix.shape)\n",
        "print(\"Embedding Vector for word \\\"me\\\" : \\n\", embedding_matrix[spam_tokenizer.word_index.get(\"me\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LO17-XELeaIV",
        "outputId": "30fb2d7b-1969-4e1a-e26b-2031471ee5a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of Embedding matrix : (4689, 50)\n",
            "Embedding Vector for word \"me\" : \n",
            " [-0.14524999  0.31265     0.15184    -0.63708001  0.63552999 -0.50295001\n",
            " -0.23214     0.52891999 -0.58629     0.53934997 -0.3055      1.03569996\n",
            " -0.77989    -0.19386999  1.22150004  0.24521001  0.26144001  0.22439\n",
            "  0.15583999 -0.79145998 -0.65262002  1.3211      0.76617998  0.38234001\n",
            "  1.44529998 -2.26430011 -1.15050006  0.50373     1.2651     -1.59029996\n",
            "  3.05180001  0.84118003 -0.69542998  0.29984999 -0.49151    -0.22312\n",
            "  0.59527999 -0.076347    0.52358001 -0.50133997  0.22483     0.01546\n",
            " -0.088005    0.21281999  0.28545001 -0.15976    -0.16777    -0.50895\n",
            "  0.14322001  1.01180005]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from keras.layers import LSTM,Dense\n",
        "#Setup Hyper Parameters for building the model\n",
        "NB_CLASSES=2\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Embedding(vocab_len,\n",
        "                                 50,\n",
        "                                 name=\"Embedding-Layer\",\n",
        "                                 weights=[embedding_matrix],\n",
        "                                 input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                 trainable=True))\n",
        "model.add(LSTM(256))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(NB_CLASSES,\n",
        "                             name='Output-Layer',\n",
        "                             activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmpcSfeLenih",
        "outputId": "24f02898-3b38-4697-ca06-60238d10f179"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " Embedding-Layer (Embedding  (None, 100, 50)           234450    \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 256)               314368    \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " Output-Layer (Dense)        (None, 2)                 514       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 549332 (2.10 MB)\n",
            "Trainable params: 549332 (2.10 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Make it verbose so we can see the progress\n",
        "VERBOSE=1\n",
        "\n",
        "#Setup Hyper Parameters for training\n",
        "BATCH_SIZE=256\n",
        "EPOCHS=10\n",
        "VALIDATION_SPLIT=0.2\n",
        "\n",
        "print(\"\\nTraining Progress:\\n------------------------------------\")\n",
        "\n",
        "history=model.fit(X_train,\n",
        "          Y_train,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=EPOCHS,\n",
        "          verbose=VERBOSE,\n",
        "          validation_split=VALIDATION_SPLIT)\n",
        "\n",
        "print(\"\\nEvaluation against Test Dataset :\\n------------------------------------\")\n",
        "model.evaluate(X_test,Y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70QCxE8ie46h",
        "outputId": "bed70cad-1455-4df9-ef16-2aa3f54de75d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Progress:\n",
            "------------------------------------\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 13s 3s/step - loss: 0.5875 - accuracy: 0.6542 - val_loss: 0.3698 - val_accuracy: 0.8875\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 8s 2s/step - loss: 0.5501 - accuracy: 0.8333 - val_loss: 0.3162 - val_accuracy: 0.8792\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.2523 - accuracy: 0.9281 - val_loss: 0.2485 - val_accuracy: 0.9167\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 7s 2s/step - loss: 0.1936 - accuracy: 0.9333 - val_loss: 0.2964 - val_accuracy: 0.8792\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 6s 2s/step - loss: 0.1765 - accuracy: 0.9365 - val_loss: 0.2390 - val_accuracy: 0.9250\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 7s 2s/step - loss: 0.1810 - accuracy: 0.9281 - val_loss: 0.2411 - val_accuracy: 0.9125\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.2086 - accuracy: 0.9292 - val_loss: 0.2016 - val_accuracy: 0.9333\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.1241 - accuracy: 0.9583 - val_loss: 0.1907 - val_accuracy: 0.9458\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 7s 2s/step - loss: 0.1242 - accuracy: 0.9542 - val_loss: 0.1860 - val_accuracy: 0.9417\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 5s 1s/step - loss: 0.1039 - accuracy: 0.9625 - val_loss: 0.1840 - val_accuracy: 0.9458\n",
            "\n",
            "Evaluation against Test Dataset :\n",
            "------------------------------------\n",
            "10/10 [==============================] - 1s 102ms/step - loss: 0.1865 - accuracy: 0.9400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.18648450076580048, 0.9399999976158142]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Two input strings to predict\n",
        "input_str=[\"Unsubscribe send GET FREE EURO STOP to 83222\",\n",
        "            \"Yup I will come over\"]\n",
        "\n",
        "#Convert to sequence using the same tokenizer as training\n",
        "input_seq = spam_tokenizer.texts_to_sequences(input_str)\n",
        "#Pad the input\n",
        "input_padded = pad_sequences(input_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "#Predict using model\n",
        "prediction=np.argmax( model.predict(input_padded), axis=1 )\n",
        "print(\"Prediction Output:\" , prediction)\n",
        "\n",
        "#Print prediction classes\n",
        "print(\"Prediction Classes are \", label_encoder.inverse_transform(prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfFq4ZzKfVD0",
        "outputId": "14a0bee3-e3ea-49aa-93ab-cc249574d8a4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 43ms/step\n",
            "Prediction Output: [1 0]\n",
            "Prediction Classes are  ['spam' 'ham']\n"
          ]
        }
      ]
    }
  ]
}